{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Set JAVA_HOME to Java 17 which is already installed.\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] += os.pathsep + os.path.join(os.environ[\"JAVA_HOME\"], \"bin\")\n",
        "\n",
        "#Install the required libraries\n",
        "!pip install pyspark\n",
        "# Initialize a Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Stop any existing Spark session to ensure new configurations take effect\n",
        "if 'spark' in locals() and spark is not None:\n",
        "    spark.stop()\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"04_clustering_and_anomaly_pyspark\")\n",
        "    .master(\"local[*]\")\n",
        "    .config(\"spark.driver.memory\", \"6g\")\n",
        "    .config(\"spark.executor.memory\", \"6g\")\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"8\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OowTtU4MUHC",
        "outputId": "757b299c-f0af-4d25-e02a-f24a08224b99"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.0.1)\n",
            "Requirement already satisfied: py4j==0.10.9.9 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--featured_parquet\", default=\"/content/data/featured.paraquet\")\n",
        "parser.add_argument(\"--out_dir\", default=\"/content/results\")\n",
        "parser.add_argument(\"--k\", type=int, default=5)\n",
        "parser.add_argument(\"--sample_fraction_for_anomaly\", type=float, default=0.05)\n",
        "\n",
        "args = parser.parse_args(args=[])"
      ],
      "metadata": {
        "id": "X-qOojCkPkyM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.parquet(args.featured_parquet)\n",
        "df = df.filter(F.col(\"features\").isNotNull()).cache()\n",
        "print(\"Loaded features count:\", df.count())"
      ],
      "metadata": {
        "id": "NQOxSyvQS4fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48b1eabb-85bf-49f9-bd98-a2ac24fc50bd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded features count: 2499784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.functions import vector_to_array\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "df_clean = (\n",
        "    df\n",
        "    .withColumn(\"features_arr\", vector_to_array(\"features\"))\n",
        "    .filter(\n",
        "        ~F.exists(\"features_arr\", lambda x: F.isnan(x))\n",
        "    )\n",
        "    .drop(\"features_arr\")\n",
        ")\n"
      ],
      "metadata": {
        "id": "TyM4qDg1OI9C"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.functions import array_to_vector\n",
        "\n",
        "df_fixed = df.withColumn(\n",
        "    \"features\",\n",
        "    array_to_vector(\n",
        "        F.transform(\n",
        "            vector_to_array(\"features\"),\n",
        "            lambda x: F.when(F.isnan(x), F.lit(0.0)).otherwise(x)\n",
        "        )\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "id": "QKky2isKOcwj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(\n",
        "    k=args.k,\n",
        "    featuresCol=\"features\",\n",
        "    seed=42,\n",
        "    maxIter=20\n",
        ")\n",
        "\n",
        "km_model = kmeans.fit(df_fixed)\n"
      ],
      "metadata": {
        "id": "qRhg-KvNOVTo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml.linalg import VectorUDT\n",
        "from pyspark.sql.functions import udf\n",
        "import math\n",
        "\n",
        "def has_nan_or_inf(v):\n",
        "    return any(math.isnan(x) or math.isinf(x) for x in v)\n",
        "\n",
        "has_bad = udf(has_nan_or_inf, \"boolean\")\n",
        "\n",
        "df.select(has_bad(col(\"features\")).alias(\"bad\")).groupBy(\"bad\").count().show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUkwlD3-QS-V",
        "outputId": "d4e95b9b-8be4-490a-e522-3fb26f564ea5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+\n",
            "|  bad|  count|\n",
            "+-----+-------+\n",
            "| true|2499758|\n",
            "|false|     26|\n",
            "+-----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import ArrayType, IntegerType\n",
        "\n",
        "def bad_indices(v):\n",
        "    return [i for i, x in enumerate(v) if math.isnan(x) or math.isinf(x)]\n",
        "\n",
        "bad_idx_udf = udf(bad_indices, ArrayType(IntegerType()))\n",
        "\n",
        "df.select(bad_idx_udf(col(\"features\")).alias(\"bad_idx\")) \\\n",
        "  .where(\"size(bad_idx) > 0\") \\\n",
        "  .limit(5) \\\n",
        "  .show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P288t0gBR2Mm",
        "outputId": "dc720637-1918-4c16-a73d-1716358c59fc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|bad_idx |\n",
            "+--------+\n",
            "|[14, 15]|\n",
            "|[14, 15]|\n",
            "|[14, 15]|\n",
            "|[14, 15]|\n",
            "|[14, 15]|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9B2PeYCSiXh"
      },
      "source": [
        "# Based on the SparkRuntimeException from a failed KMeans run, the 'features' vector has 78 dimensions.\n",
        "# Define feature_cols as a list of indices for these dimensions.\n",
        "feature_cols = list(range(78))\n",
        "\n",
        "feature_cols_fixed = [\n",
        "    c for i, c in enumerate(feature_cols)\n",
        "    if i not in (14, 15)\n",
        "]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, c in enumerate(feature_cols):\n",
        "    if i in (14, 15):\n",
        "        print(i, c)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJs8tDBcTNcr",
        "outputId": "a4c63aaa-a0ab-454b-dba5-f01b2476b9ef"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14 14\n",
            "15 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(df\n",
        ".withColumn(\"feature_14\", F.element_at(vector_to_array(\"features\"), 15))\n",
        ".withColumn(\"feature_15\", F.element_at(vector_to_array(\"features\"), 16))\n",
        ".select(\"feature_14\", \"feature_15\").summary().show())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OfUNm5gTVpq",
        "outputId": "4ca3b528-1f4e-40f0-bbdb-717b83446069"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+----------+\n",
            "|summary|feature_14|feature_15|\n",
            "+-------+----------+----------+\n",
            "|  count|   2499784|   2499784|\n",
            "|   mean|       NaN|       NaN|\n",
            "| stddev|       NaN|       NaN|\n",
            "|    min|       0.0|       0.0|\n",
            "|    25%|       NaN|       NaN|\n",
            "|    50%|       NaN|       NaN|\n",
            "|    75%|       NaN|       NaN|\n",
            "|    max|       NaN|       NaN|\n",
            "+-------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bad_cols = [\"feature_14\", \"feature_15\"]\n",
        "\n",
        "df_fixed = df.drop(*bad_cols)\n"
      ],
      "metadata": {
        "id": "--bXdY4LUct0"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cols_fixed = [c for c in feature_cols if c not in bad_cols]\n"
      ],
      "metadata": {
        "id": "C3I8zC7WUegx"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cols_fixed = [i for i in range(78) if i not in (14, 15)]"
      ],
      "metadata": {
        "id": "AqnaC1ZtU5kh"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.functions import vector_to_array, array_to_vector\n",
        "from pyspark.sql.functions import array, col, lit\n",
        "\n",
        "# Convert the existing 'features' vector to an array\n",
        "df_with_array = df.withColumn(\"features_array\", vector_to_array(\"features\"))\n",
        "\n",
        "# Create a new array by selecting elements based on feature_cols_fixed (which should now be integers)\n",
        "new_features_array_expr = array([F.element_at(col(\"features_array\"), lit(idx + 1)) for idx in feature_cols_fixed])\n",
        "\n",
        "df_fixed = df_with_array.withColumn(\"features\", array_to_vector(new_features_array_expr)).drop(\"features_array\")"
      ],
      "metadata": {
        "id": "Ca7l9GeGVKNL"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.ml.linalg import VectorUDT\n",
        "\n",
        "def has_nan_or_inf(v):\n",
        "    return any(math.isnan(x) or math.isinf(x) for x in v)\n",
        "\n",
        "has_bad = udf(has_nan_or_inf, \"boolean\")\n",
        "\n",
        "df_fixed.select(has_bad(col(\"features\")).alias(\"bad\")) \\\n",
        "    .groupBy(\"bad\").count().show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLGP-j60Vnky",
        "outputId": "6d654e79-65b4-4b6a-e9d4-d147515fd0ba"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+\n",
            "|  bad|  count|\n",
            "+-----+-------+\n",
            "|false|2499784|\n",
            "+-----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "\n",
        "kmeans = KMeans(\n",
        "    k=args.k,\n",
        "    featuresCol=\"features\",\n",
        "    seed=42,\n",
        "    predictionCol=\"cluster\"\n",
        ")\n",
        "\n",
        "km_model = kmeans.fit(df_fixed)\n",
        "df_k = km_model.transform(df_fixed)\n",
        "\n",
        "evaluator = ClusteringEvaluator(\n",
        "    featuresCol=\"features\",\n",
        "    predictionCol=\"cluster\",\n",
        "    metricName=\"silhouette\",\n",
        "    distanceMeasure=\"squaredEuclidean\"\n",
        ")\n",
        "\n",
        "sil = evaluator.evaluate(df_k)\n",
        "print(\"KMeans silhouette score:\", sil)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJ0kl3Y5W1na",
        "outputId": "c9db3770-8bfb-422c-e219-6533ee3669a7"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KMeans silhouette score: 0.1257080671813052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_counts = df_k.groupBy(\"cluster\").count().orderBy(\"cluster\").toPandas()\n",
        "cluster_counts.to_csv(os.path.join(args.out_dir, \"kmeans_cluster_counts.csv\"), index=False)\n",
        "print(\"Saved kmeans_cluster_counts.csv\")"
      ],
      "metadata": {
        "id": "hSv_Pe8aTcgk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc911838-cbdb-481c-e659-dd6218c7edbb"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved kmeans_cluster_counts.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from pyspark.ml.iforest import IForest\n",
        "    print(\"Using spark-iforest IForest for anomaly detection\")\n",
        "    iso = IForest(contamination=0.01, featuresCol=\"features\", predictionCol=\"anomaly\", scoreCol=\"anomaly_score\")\n",
        "    iso_model = iso.fit(df)\n",
        "    df_iso = iso_model.transform(df)\n",
        "    df_iso.select(\"anomaly\", \"anomaly_score\").limit(10).show()\n",
        "    df_iso.select(\"anomaly\", \"anomaly_score\").write.mode(\"overwrite\").parquet(os.path.join(args.out_dir, \"iforest_scores.parquet\"))\n",
        "    print(\"Saved iforest anomaly scores parquet.\")\n",
        "except Exception as e:\n",
        "    print(\"spark-iforest not available or failed:\", e)\n",
        "    print(\"Falling back to sklearn IsolationForest on a sample (converted to pandas) — only recommended on a sample.\")\n",
        "    sample = df.sample(withReplacement=False, fraction=args.sample_fraction_for_anomaly, seed=42)\n",
        "    pdf = sample.select(\"features\").toPandas()\n",
        "    import numpy as np\n",
        "    X = np.vstack(pdf['features'].apply(lambda v: v.toArray()).values)\n",
        "    from sklearn.ensemble import IsolationForest\n",
        "    iso_sklearn = IsolationForest(n_estimators=200, contamination=0.01, random_state=42)\n",
        "    iso_sklearn.fit(X)\n",
        "    scores = -iso_sklearn.decision_function(X)\n",
        "    out_pdf = sample.withColumn(\"tmp_id\", F.monotonically_increasing_id()).toPandas()\n",
        "    out_pdf['anomaly_score'] = scores\n",
        "    out_pdf[['anomaly_score']].to_csv(os.path.join(args.out_dir, \"isolation_forest_sample_scores.csv\"), index=False)\n",
        "    print(\"Saved sample anomaly scores to results/\")"
      ],
      "metadata": {
        "id": "zuDkxAM2Tdil",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc872af1-5019-42b1-9da3-429277fe4c39"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spark-iforest not available or failed: No module named 'pyspark.ml.iforest'\n",
            "Falling back to sklearn IsolationForest on a sample (converted to pandas) — only recommended on a sample.\n",
            "Saved sample anomaly scores to results/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "b4tDDM8KTuF8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}