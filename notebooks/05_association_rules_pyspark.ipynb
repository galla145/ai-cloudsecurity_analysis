{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TPNBipJ_Oegy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d1f5fad-46fa-458e-fc2e-a97715d4e953"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.0.1)\n",
            "Requirement already satisfied: py4j==0.10.9.9 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.9)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Set JAVA_HOME to Java 17 which is already installed.\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] += os.pathsep + os.path.join(os.environ[\"JAVA_HOME\"], \"bin\")\n",
        "\n",
        "#Install the required libraries\n",
        "!pip install pyspark\n",
        "# Initialize a Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Stop any existing Spark session to ensure new configurations take effect\n",
        "if 'spark' in locals() and spark is not None:\n",
        "    spark.stop()\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"05_association_rules_pyspark\")\n",
        "    .master(\"local[*]\")\n",
        "    .config(\"spark.driver.memory\", \"6g\")\n",
        "    .config(\"spark.executor.memory\", \"6g\")\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"8\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "spark.sparkContext.setLogLevel(\"WARN\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.fpm import FPGrowth\n",
        "from pyspark.ml.feature import QuantileDiscretizer\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--featured_parquet\", default=\"/content/data/featured.paraquet\")\n",
        "parser.add_argument(\"--out_dir\", default=\"/content/results\")\n",
        "parser.add_argument(\"--num_bins\", type=int, default=3)\n",
        "parser.add_argument(\"--min_support\", type=float, default=0.01)\n",
        "\n",
        "args = parser.parse_args(args=[])"
      ],
      "metadata": {
        "id": "WXQQZMbNQMgL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.parquet(args.featured_parquet)\n",
        "print(\"Loaded df count:\", df.count())"
      ],
      "metadata": {
        "id": "hKZBDrC4T3HT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0f14efb-79c5-4572-a539-74464716cbaa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded df count: 2499784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = [c for c,t in df.dtypes if t in ('double','int','long','float','bigint','tinyint','smallint','decimal')]"
      ],
      "metadata": {
        "id": "Z5vCHXGtT8zj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = [c for c in numeric_cols if c not in ('y_binary',)]"
      ],
      "metadata": {
        "id": "iVV4VfyuUOsc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = df.sample(False, 0.05, seed=42)\n",
        "stats = sample.select([F.variance(c).alias(c) for c in numeric_cols]).collect()[0].asDict()\n",
        "sorted_vars = sorted(stats.items(), key=lambda x: (x[1] is None, -float(x[1]) if x[1] is not None else 0))\n",
        "chosen = [c for c,_ in sorted_vars[:10]]\n",
        "print(\"Chosen numeric cols for discretization:\", chosen)"
      ],
      "metadata": {
        "id": "E55ardsnUTmg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81780483-fa05-439d-b2ea-73961020022b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chosen numeric cols for discretization: ['Flow Duration', 'Fwd IAT Total', 'Bwd IAT Total', 'Fwd IAT Max', 'Flow IAT Max', 'Bwd IAT Max', 'Fwd IAT Std', 'Fwd IAT Mean', 'Bwd IAT Mean', 'Fwd IAT Min']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "binned_cols = []\n",
        "for c in chosen:\n",
        "    out_col = c + \"_bin\"\n",
        "    discretizer = QuantileDiscretizer(numBuckets=args.num_bins, inputCol=c, outputCol=out_col, relativeError=0.01)\n",
        "    df = discretizer.fit(df).transform(df)\n",
        "    # map numeric bin indices to string items like col_bin_0\n",
        "    df = df.withColumn(out_col + \"_item\", F.concat(F.lit(c + \"_bin_\"), F.col(out_col).cast(\"int\").cast(\"string\")))\n",
        "    binned_cols.append(out_col + \"_item\")"
      ],
      "metadata": {
        "id": "cfDcqj7sUbIh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_cols = [c for c,t in df.dtypes if c.endswith(\"_ohe\") or (t=='int' and df.select(c).distinct().count()==2)]"
      ],
      "metadata": {
        "id": "IzyhkPgNUgno"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_cols = []\n",
        "print(\"binned columns items count:\", len(binned_cols))"
      ],
      "metadata": {
        "id": "vpaPbuvzVWsk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "745b5171-eaf2-4744-b205-b18ebeac781d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "binned columns items count: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_items = df.select(*binned_cols).withColumn(\"items\", F.array(*[F.col(c) for c in binned_cols])).select(\"items\")\n",
        "df_items = df_items.na.drop()\n",
        "print(\"Prepared transactions count:\", df_items.count())"
      ],
      "metadata": {
        "id": "3eu6ofjBVbSG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07ebbcc6-3b74-4d15-a210-b79421209460"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared transactions count: 2499784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fp = FPGrowth(itemsCol=\"items\", minSupport=args.min_support, minConfidence=0.6)\n",
        "model = fp.fit(df_items)\n",
        "freq_itemsets = model.freqItemsets\n",
        "rules = model.associationRules"
      ],
      "metadata": {
        "id": "iQJEUeBmVgA7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(args.out_dir, exist_ok=True)\n",
        "freq_itemsets.coalesce(1).write.mode(\"overwrite\").parquet(os.path.join(args.out_dir, \"fp_freq_itemsets.parquet\"))\n",
        "rules.coalesce(1).write.mode(\"overwrite\").parquet(os.path.join(args.out_dir, \"fp_rules.parquet\"))\n",
        "print(\"Saved frequent itemsets and rules to results/\")"
      ],
      "metadata": {
        "id": "YjFB2YvvVj2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a38cbc54-d01b-476c-98eb-76cfa56096df"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved frequent itemsets and rules to results/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rules.orderBy(F.desc(\"confidence\")).limit(100).toPandas().to_csv(os.path.join(args.out_dir, \"association_rules_top100.csv\"), index=False)\n",
        "print(\"Saved association_rules_top100.csv\")"
      ],
      "metadata": {
        "id": "P9TsWSRHVqvr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf6fd825-c768-4462-b933-01dd4a43d1e7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved association_rules_top100.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "b443KDwwVrya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XHGgTRGUVvAi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}