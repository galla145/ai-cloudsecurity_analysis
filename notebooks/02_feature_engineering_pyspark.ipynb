{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Install the required libraries\n",
        "!pip install pyspark\n",
        "!pip install spark\n",
        "from pyspark.sql import SparkSession\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"02_feature_engineering_pyspark\").getOrCreate()\n",
        "spark.sparkContext.setLogLevel(\"WARN\")"
      ],
      "metadata": {
        "id": "rL_tSksIMGkM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cf4c98c-0093-45e1-fad0-db9094c5b348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.0.1)\n",
            "Requirement already satisfied: py4j==0.10.9.9 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.9)\n",
            "Requirement already satisfied: spark in /usr/local/lib/python3.12/dist-packages (0.3.2)\n",
            "Requirement already satisfied: aiohttp>=3.10.5 in /usr/local/lib/python3.12/dist-packages (from spark) (3.13.2)\n",
            "Requirement already satisfied: aiosqlite>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from spark) (0.21.0)\n",
            "Requirement already satisfied: docstring-parser>=0.17.0 in /usr/local/lib/python3.12/dist-packages (from spark) (0.17.0)\n",
            "Requirement already satisfied: jinja2>=3.1.6 in /usr/local/lib/python3.12/dist-packages (from spark) (3.1.6)\n",
            "Requirement already satisfied: json-repair>=0.52.4 in /usr/local/lib/python3.12/dist-packages (from spark) (0.54.2)\n",
            "Requirement already satisfied: openai>=2.7.1 in /usr/local/lib/python3.12/dist-packages (from spark) (2.9.0)\n",
            "Requirement already satisfied: pydantic<3.0,>=2.7 in /usr/local/lib/python3.12/dist-packages (from spark) (2.12.3)\n",
            "Requirement already satisfied: starlette>=0.50.0 in /usr/local/lib/python3.12/dist-packages (from spark) (0.50.0)\n",
            "Requirement already satisfied: typing-extensions>=4.15.0 in /usr/local/lib/python3.12/dist-packages (from spark) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.38.0 in /usr/local/lib/python3.12/dist-packages (from spark) (0.38.0)\n",
            "Requirement already satisfied: websockets>=15.0 in /usr/local/lib/python3.12/dist-packages (from spark) (15.0.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.5->spark) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.5->spark) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.5->spark) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.5->spark) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.5->spark) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.5->spark) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.5->spark) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=3.1.6->spark) (3.0.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai>=2.7.1->spark) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=2.7.1->spark) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai>=2.7.1->spark) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai>=2.7.1->spark) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=2.7.1->spark) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai>=2.7.1->spark) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->spark) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->spark) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->spark) (0.4.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.38.0->spark) (8.3.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.38.0->spark) (0.16.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai>=2.7.1->spark) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai>=2.7.1->spark) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai>=2.7.1->spark) (1.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8wTdsawAL6Br"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import argparse\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--input_parquet\", default=\"/content/data/cleaned.paraquet\")\n",
        "parser.add_argument(\"--out_parquet\", default=\"/content/data/featured.paraquet\")\n",
        "args = parser.parse_args(args=[])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.parquet(args.input_parquet)\n",
        "print(\"Loaded df shape (approx):\", df.count(), len(df.columns))"
      ],
      "metadata": {
        "id": "U4vG6EolMZ6m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "938b8ece-7ae0-4191-a389-4a11135469c9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded df shape (approx): 2499784 79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cols = df.columns\n",
        "if set(['flow_duration','tot_bytes']).issubset(set(cols)):\n",
        "    df = df.withColumn(\"bytes_per_sec\", (F.col(\"tot_bytes\") / (F.col(\"flow_duration\") + F.lit(1.0))).cast(DoubleType()))\n",
        "if set(['tot_bytes','tot_fwd_packets','tot_bwd_packets']).issubset(set(cols)):\n",
        "    df = df.withColumn(\"avg_packet_size\", (F.col(\"tot_bytes\") / (F.col(\"tot_fwd_packets\") + F.col(\"tot_bwd_packets\") + F.lit(1.0))).cast(DoubleType()))\n",
        "if set(['tot_fwd_packets','tot_bwd_packets']).issubset(set(cols)):\n",
        "    df = df.withColumn(\"fwd_pkt_ratio\", (F.col(\"tot_fwd_packets\") / (F.col(\"tot_fwd_packets\") + F.col(\"tot_bwd_packets\") + F.lit(1.0))).cast(DoubleType()))\n"
      ],
      "metadata": {
        "id": "obBSS9iiMnKx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string_cols = [c for c,t in df.dtypes if t == 'string']"
      ],
      "metadata": {
        "id": "gCkAvcTdMvl_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "candidates = []\n",
        "for c in string_cols:\n",
        "    distinct = df.select(c).distinct().count()\n",
        "    if distinct <= 50:\n",
        "        candidates.append(c)\n",
        "print(\"Categorical columns to OHE (<=50 cardinality):\", candidates)"
      ],
      "metadata": {
        "id": "A5c0GYMvMwnu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3723370d-2a0d-4374-ce6e-360e10513655"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categorical columns to OHE (<=50 cardinality): []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indexers = [StringIndexer(inputCol=c, outputCol=c+\"_idx\", handleInvalid=\"keep\") for c in candidates]\n",
        "encoders = [OneHotEncoder(inputCol=c+\"_idx\", outputCol=c+\"_ohe\") for c in candidates]"
      ],
      "metadata": {
        "id": "9nxjs04VM2C9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = [c for c,t in df.dtypes if t in ('double','int','long','float','bigint','tinyint','smallint','decimal')]\n",
        "numeric_cols = [c for c in numeric_cols if c not in ['y_binary']]  # exclude label"
      ],
      "metadata": {
        "id": "lrxdGttoM78L"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ohe_cols = [c+\"_ohe\" for c in candidates]\n",
        "feature_cols = numeric_cols + ohe_cols\n",
        "print(\"Feature column count:\", len(feature_cols))"
      ],
      "metadata": {
        "id": "H6UtNVOmNIyx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4d5a504-e486-4f8b-ec92-6f7b3ca6947a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature column count: 78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_assembled\", handleInvalid=\"keep\")\n",
        "scaler = StandardScaler(inputCol=\"features_assembled\", outputCol=\"features\", withStd=True, withMean=False)\n"
      ],
      "metadata": {
        "id": "Fn3dPazSNS2I"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_stages = []\n",
        "pipeline_stages.extend(indexers)\n",
        "pipeline_stages.extend(encoders)\n",
        "pipeline_stages.append(assembler)\n",
        "pipeline_stages.append(scaler)\n",
        "pipeline = Pipeline(stages=pipeline_stages)"
      ],
      "metadata": {
        "id": "5D1DKwEANTjK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Fitting pipeline...\")\n",
        "model = pipeline.fit(df)\n",
        "df_trans = model.transform(df)"
      ],
      "metadata": {
        "id": "AuT4FKrHNYiR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "431aba46-67b3-4bcf-cec5-9b2a89c9f97b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting pipeline...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"models\", exist_ok=True)\n",
        "model.write().overwrite().save(\"models/feature_pipeline\")\n",
        "print(\"Saved feature pipeline to models/feature_pipeline\")"
      ],
      "metadata": {
        "id": "7ASXEL7XNdC7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e7cde11-e53e-44e6-c8c7-731a20e96040"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved feature pipeline to models/feature_pipeline\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_trans.select(\n",
        "    *df.columns,\n",
        "    \"features\",\n",
        "    col(\"y_binary\").alias(\"y_binary_new\")\n",
        ").write.mode(\"overwrite\").parquet(args.out_parquet)\n",
        "\n"
      ],
      "metadata": {
        "id": "n5x7Ah2viirO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "uvEjG9OWNoFT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}