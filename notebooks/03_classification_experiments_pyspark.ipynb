{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Q2M8_siZOKFq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "748d73df-596d-42b7-c547-250bdace04b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.0.1)\n",
            "Requirement already satisfied: py4j==0.10.9.9 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.9)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Set JAVA_HOME to Java 17 which is already installed.\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] += os.pathsep + os.path.join(os.environ[\"JAVA_HOME\"], \"bin\")\n",
        "\n",
        "#Install the required libraries\n",
        "!pip install pyspark\n",
        "# Initialize a Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Stop any existing Spark session to ensure new configurations take effect\n",
        "if 'spark' in locals() and spark is not None:\n",
        "    spark.stop()\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"03_classification_experiments_pyspark\")\n",
        "    .master(\"local[*]\")\n",
        "    .config(\"spark.driver.memory\", \"8g\")   # Increased from 6g\n",
        "    .config(\"spark.executor.memory\", \"6g\")\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"8\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"WARN\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml import PipelineModel\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "import pyspark.sql.functions as F\n",
        "import json\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--featured_parquet\", default=\"/content/data/featured.paraquet\")\n",
        "parser.add_argument(\"--out_dir\", default=\"/content/results\")\n",
        "args = parser.parse_args(args=[])"
      ],
      "metadata": {
        "id": "EtuFAvKOO5aA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.parquet(args.featured_parquet)\n",
        "print(\"Loaded featured data count:\", df.count())\n"
      ],
      "metadata": {
        "id": "pAmG09ogQ3xs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81324d36-4f2c-492d-f37d-0ec378092ea1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded featured data count: 2499784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'y_binary' not in df.columns:\n",
        "    raise RuntimeError(\"y_binary target not found in featured data. Run preprocessing to create label.\")\n"
      ],
      "metadata": {
        "id": "WbWYzHqcRJmL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.filter(F.col(\"features\").isNotNull())"
      ],
      "metadata": {
        "id": "s6m2Xue-RSjx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = df.randomSplit([0.7, 0.3], seed=42)\n",
        "print(\"Train/Test counts:\", train.count(), test.count())"
      ],
      "metadata": {
        "id": "UcKFK7J5RTn_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "158f94ac-f88e-475f-ea08-f7052bdd3727"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/Test counts: 1749381 750403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.repartition(4).cache()\n",
        "train.count()   # materialize cache\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qMOLmIG6b87",
        "outputId": "9402d55c-37a8-445e-e622-1d5fc0e2f546"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1749381"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"y_binary\", probabilityCol=\"probability\", rawPredictionCol=\"rawPrediction\", predictionCol=\"prediction\", seed=42, numTrees=100)"
      ],
      "metadata": {
        "id": "_fbkpz8XRZBm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paramGrid = (ParamGridBuilder()\n",
        "             .addGrid(rf.maxDepth, [10, 20])\n",
        "             .addGrid(rf.numTrees, [100])\n",
        "             .build())\n"
      ],
      "metadata": {
        "id": "SY7OeVmKRfDS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = BinaryClassificationEvaluator(labelCol=\"y_binary\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "cv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3, parallelism=2)\n"
      ],
      "metadata": {
        "id": "sExXRr75Rj3P"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CrossValidator(\n",
        "    estimator=rf,\n",
        "    estimatorParamMaps=paramGrid,\n",
        "    evaluator=evaluator,\n",
        "    numFolds=3,        # NOT 5\n",
        "    parallelism=1     # VERY IMPORTANT\n",
        ")\n"
      ],
      "metadata": {
        "id": "KA3yrZZz6jN0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"y_binary\",\n",
        "    numTrees=50,        # not 200\n",
        "    maxDepth=5,         # not 10+\n",
        "    subsamplingRate=0.7\n",
        ")\n"
      ],
      "metadata": {
        "id": "_m8luyl26pHK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "from pyspark.sql.types import BooleanType\n",
        "from pyspark.sql.functions import udf\n",
        "import math\n",
        "\n",
        "# Re-define rf with smaller parameters to reduce memory footprint\n",
        "rf = RandomForestClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"y_binary\",\n",
        "    probabilityCol=\"probability\",\n",
        "    rawPredictionCol=\"rawPrediction\",\n",
        "    predictionCol=\"prediction\",\n",
        "    seed=42,\n",
        "    numTrees=50,       # Reduced from 100\n",
        "    maxDepth=5         # Reduced from [10, 20] search space\n",
        ")\n",
        "\n",
        "# Re-define paramGrid with a smaller, single set of parameters for testing\n",
        "# This significantly reduces the memory consumption during cross-validation.\n",
        "paramGrid = (ParamGridBuilder()\n",
        "             .addGrid(rf.maxDepth, [5])\n",
        "             .addGrid(rf.numTrees, [50])\n",
        "             .build())\n",
        "\n",
        "# Re-define cv with the new rf and paramGrid\n",
        "# Ensure parallelism is set to 1 to avoid concurrent model training,\n",
        "# which can exacerbate OutOfMemory errors.\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"y_binary\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "cv = CrossValidator(\n",
        "    estimator=rf,\n",
        "    estimatorParamMaps=paramGrid,\n",
        "    evaluator=evaluator,\n",
        "    numFolds=3,\n",
        "    parallelism=1\n",
        ")\n",
        "\n",
        "def has_nan_or_inf(v):\n",
        "    if v is None: # Handle cases where the vector itself might be null, though previously filtered\n",
        "        return True # Treat null vectors as invalid for this check\n",
        "    for val in v.toArray():\n",
        "        if math.isnan(val) or math.isinf(val):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "has_nan_or_inf_udf = udf(has_nan_or_inf, BooleanType())\n",
        "\n",
        "# Filter 'train' DataFrame to remove rows with NaN/Infinity in 'features' vector\n",
        "# Only do this if it hasn't been done to the parent 'df' already\n",
        "train = train.filter(~has_nan_or_inf_udf(train['features']))\n",
        "\n",
        "cvModel = cv.fit(train)\n",
        "bestModel = cvModel.bestModel\n",
        "print(\"Best model params:\", bestModel._java_obj.getMaxDepth(), bestModel.getNumTrees)"
      ],
      "metadata": {
        "id": "zKcvJkifRtBV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f33e286-9fab-4bf6-a236-295d25382571"
      },
      "execution_count": 15,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best model params: 5 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = bestModel.transform(test)\n",
        "bauc = evaluator.evaluate(preds)\n",
        "mce = MulticlassClassificationEvaluator(labelCol=\"y_binary\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "f1 = mce.evaluate(preds)\n",
        "precision_ev = MulticlassClassificationEvaluator(labelCol=\"y_binary\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
        "recall_ev = MulticlassClassificationEvaluator(labelCol=\"y_binary\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
        "prec = precision_ev.evaluate(preds)\n",
        "rec = recall_ev.evaluate(preds)"
      ],
      "metadata": {
        "id": "Uv22TJF8RyCJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = {\"roc_auc\": bauc, \"f1\": f1, \"precision\": prec, \"recall\": rec}\n",
        "os.makedirs(args.out_dir, exist_ok=True)\n",
        "with open(os.path.join(args.out_dir, \"rf_metrics.json\"), \"w\") as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "print(\"Saved metrics:\", metrics)"
      ],
      "metadata": {
        "id": "r0SVUtVlR4-X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36fcefc5-5d21-4bc6-9751-82265168df96"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved metrics: {'roc_auc': 0.6666578594716507, 'f1': 0.8744697893318645, 'precision': 0.872267851304158, 'recall': 0.8781228220036433}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = os.path.join(\"models\", \"rf_spark_model\")\n",
        "bestModel.write().overwrite().save(model_path)\n",
        "print(\"Saved best RF model to:\", model_path)"
      ],
      "metadata": {
        "id": "eFvC1ZGZR_4B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "751f922b-360d-441e-e304-033d0a07bb74"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved best RF model to: models/rf_spark_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds.select(\"prediction\", \"probability\", \"y_binary\").limit(1000).toPandas().to_csv(os.path.join(args.out_dir, \"rf_preds_sample.csv\"), index=False)\n",
        "print(\"Saved predictions sample to results/\")"
      ],
      "metadata": {
        "id": "BoMQyC7JSAyz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fe7ce45-9dea-4a89-c877-9dbd78fed6b8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved predictions sample to results/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "TVqjb5I1SFXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "87kVtYqMSJq3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}